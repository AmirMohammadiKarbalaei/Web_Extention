{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from lxml import etree\n",
    "import json\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances,manhattan_distances\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sitemaps_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"0\"] = {'upvotes': 0, 'downvotes': 0}\n",
    "dic[\"1\"] = {'upvotes': 1, 'downvotes': 0}\n",
    "dic[\"2\"] = {'upvotes': 2, 'downvotes': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect Preferences please\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mI\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnews_idx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     10\u001b[0m related_articles\u001b[38;5;241m.\u001b[39mappend(I[news_idx][\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "most_upvoted = sorted(dic.items(), key=lambda x: x[1]['upvotes'], reverse=True)\n",
    "sorted_upvoted_idxs = [i[0] for idx, i in enumerate(most_upvoted) if i[1][\"upvotes\"] > 0]\n",
    "related_articles = []\n",
    "prefrences = [\"sport\",\"weather\"]\n",
    "for news_idx in sorted_upvoted_idxs:\n",
    "    if articles_df.topic[int(news_idx)] not in prefrences:\n",
    "            print(\"Select Preferences please\")\n",
    "            continue\n",
    "    related_articles.append(I[int(news_idx)][1:])\n",
    "\n",
    "#for articles in related_articles:\n",
    "      \n",
    "    \n",
    "    \n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BBC URLs: 429\n",
      "Number of Sky URLs: 18\n"
     ]
    }
   ],
   "source": [
    "BBC_news_sitemaps = [\"https://www.bbc.com/sitemaps/https-sitemap-com-news-1.xml\",\n",
    "                     \"https://www.bbc.com/sitemaps/https-sitemap-com-news-2.xml\",\n",
    "                     \"https://www.bbc.com/sitemaps/https-sitemap-com-news-3.xml\"]\n",
    "\n",
    "sky_news_sitemaps = [\"https://news.sky.com/sitemap/sitemap-news.xml\",\n",
    "                    \"https://www.skysports.com/sitemap/sitemap-news.xml\"]\n",
    "\n",
    "namespaces = {\n",
    "    'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9',\n",
    "    'news': 'http://www.google.com/schemas/sitemap-news/0.9'\n",
    "}\n",
    "\n",
    "urls = {}\n",
    "# BBC News uses 'sitemap:lastmod' for the date tag\n",
    "urls[\"bbc\"] = Extract_todays_urls_from_sitemaps(BBC_news_sitemaps, namespaces, 'sitemap:lastmod')\n",
    "print(f\"Number of BBC URLs: {len(urls['bbc'])}\")\n",
    "\n",
    "# Sky News uses 'news:publication_date' for the date tag\n",
    "urls[\"sky\"] = Extract_todays_urls_from_sitemaps(sky_news_sitemaps, namespaces, 'news:publication_date')\n",
    "print(f\"Number of Sky URLs: {len(urls['sky'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langid.langid:initializing identifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ bbc ------\n",
      "Topic\n",
      "news         125\n",
      "sport         34\n",
      "weather       24\n",
      "newsround      5\n",
      "Total        188\n",
      "\n",
      "------ sky ------\n",
      "Topic\n",
      "story       10\n",
      "football     3\n",
      "tennis       1\n",
      "racing       1\n",
      "cricket      1\n",
      "Total        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbc_topics_to_drop = {\"pidgin\", \"hausa\", \"swahili\", \"naidheachdan\",\"videos\",\"cymrufyw\"}\n",
    "df_BBC = process_news_data(urls, \"bbc\", bbc_topics_to_drop)\n",
    "\n",
    "sky_topics_to_drop = {\"arabic\", \"urdu\"}\n",
    "df_Sky = process_news_data(urls, \"sky\", sky_topics_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (918239044.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    titles to drop: [\"One-minute World News\",\"\"]\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "titles to drop: [\"One-minute World News\",\"\"]\n",
    "\n",
    "title contains \"weekly\" : df_BBC[df_BBC['Title'].str.contains('weekly round-up', case=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Url</th>\n",
       "      <th>Last Modified</th>\n",
       "      <th>Title</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.bbc.com/newsround/66176104</td>\n",
       "      <td>2024-05-25T08:25:50Z</td>\n",
       "      <td>Watch Newsround</td>\n",
       "      <td>newsround</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Url         Last Modified  \\\n",
       "106  https://www.bbc.com/newsround/66176104  2024-05-25T08:25:50Z   \n",
       "\n",
       "               Title      Topic  \n",
       "106  Watch Newsround  newsround  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BBC[df_BBC['Title']==\"Watch Newsround\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BBC[~df_BBC['Title'].str.contains('weekly round-up', case=False)].drop_duplicates(\"Title\").reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-minute World News\n",
      "South America weather forecast\n",
      "General election 2024 poll tracker: How do the parties compare?\n",
      "RideLondon: Gas leak near finish line in Maldon\n",
      "Michael Gove steps down in mass exodus of MPs before election\n",
      "Defence Secretary Lloyd Austin to transfers powers due to medical procedure\n",
      "Childbirth: Wrexham woman left with stoma and PTSD\n",
      "Neurodivergent and queer author on purging Llanelli memories\n",
      "Football gossip: McKenna, Ten Hag, Pochettino, Maresca, Sancho, Salah, Kompany\n",
      "Extinct ‘mountain jewel’ plant returned to wild - in secret location\n",
      "Smartphone ban for kids is worth considering - MPs\n",
      "Papua New Guinea landslide: Race to rescue villagers trapped\n",
      "Royal Mail investigated by Ofcom for missing delivery targets\n",
      "Bournemouth stabbing at beach leaves woman dead\n",
      "South Africa elections: Some are losing faith in the ANC\n",
      "Rudimental's Locksmith meets 10-year-old carer at festival\n",
      "Gaza war: Israelis attack aid convoys sent for Palestinians\n",
      "Britain's Got Talent: Comedian ‘embraces’ his tics\n",
      "General election guide to Cambridgeshire and Peterborough \n",
      "Astrea schools in Cambridgeshire given 'good' Ofsted ratings\n",
      "Colchester United boss Danny Cowley coaches daughter to success\n",
      "Incinerator plan for Basildon industrial estate is rejected\n",
      "'Reckless' driver jailed for North Elham crash which killed woman\n",
      "Dancer Marc Brew paralysed in crash 'humbled' to inspire others\n",
      "Inflation: 'I drive a lorry to keep my sweet shop going'\n",
      "Colin Firth's wet shirt from Pride and Prejudice on display in Halifax\n",
      "Northamptonshire's new fire chief says 'move on' from sexism row\n",
      "Gaping Gill: Chance to be winched down 365ft underground cave\n",
      "Sand and gravel quarry off M1 in Northamptonshire turned down\n",
      "Keira Knightley, James Norton and Bridgerton costumes go on show\n",
      "Watch: The 25-year hunt for Victoria Hall's killer\n",
      "Scarborough: Man jailed over £250,000 cannabis farm\n",
      "Ipswich linocut prints created with a road roller\n",
      "Hull breastfeeding friendly trail map launched\n",
      "Asian hornets pose 'massive risk', Lincolnshire beekeeper warns\n",
      "In It Together: Port Talbot family opens back garden for festival\n",
      "Stilton cheese rolling - what's it all about?\n",
      "Historical artefacts celebrate 50 years of Lancashire County Council\n",
      "Chorley festival to mark Astley Hall's centenary\n",
      "Merseyrail train named in honour of Cunard ahead of Queen Anne ship ceremony\n",
      "Southport Pier to remain closed for another summer\n",
      "Weapons seized in operation to tackle knife crime in Devon and Cornwall\n",
      "Jethro memorial statue to be unveiled in Liskeard\n",
      "Laptops and SIM cards to be handed out to tackle digital divide\n",
      "Hampshire & Isle of Wight's Big Picture: 20 May - 26 May 2024\n",
      "Artist's animation aims to help people with hoarding disorder\n",
      "Turner Contemporary: Sculpture highlights importance of alt text\n",
      "LS Lowry seascapes exhibition in Berwick\n",
      "Duck family march to safety at Hardwick Hall\n",
      "Sapling trees knocked down in Huntingdon park\n",
      "LV18 lightship to reopen in Harwich after fire\n",
      "Fish and chip shop rise has lasting impact on King's Lynn\n",
      "Suffolk vegetable business says 'stop growing boring' seeds\n",
      "London tram service levels abysmal, Lib Dems say\n",
      "Little Steeping shooting: Man injured in village attack\n",
      "Lewisham Council published residents' details on website\n",
      "D-Day: Normandy veteran shares stories with Romford children\n",
      "Boy, 15, dies after being struck by car in Oldham\n",
      "Perranrporth shop owners confident ahead of summer season\n",
      "Jersey environmentally friendly mooring trial extended\n",
      "Man in critical condition after being hit by car in Swindon\n",
      "Wisbech free school due to open in September\n",
      "Roland Garros: Five stories to watch in 2024 including Nadal, Djokovic, Murray, Swiatek\n",
      "DeadEndia creator gives students tips on selling at MCM Comic Con\n",
      "Brian Low: Man charged over dog walker shooting death in Aberfeldy\n",
      "Bradford stabbing: Police make fresh appeal over 1981 murder\n",
      "Scotland's papers: Protesters target Hampden and pyro bans\n",
      "St Ives defibrillator installed after police help save man\n",
      "Stoke-on-Trent adult care home to close over funding issues\n",
      "In pictures: Wildfire-hit Corrimony Nature Reserve a year on\n",
      "Scarborough: Community woodland is digitally mapped\n",
      "Lonely Smallthorne widow checks in on isolated people\n",
      "Lance Bradley: Ospreys boss says Welsh rugby wants four sides \n",
      "Free sessions as South Holderness gym reopens after makeover\n",
      "RAF Coningsby: Helicopter crash simulated for training exercise\n",
      "Leasehold reforms become law but without ground rent cap\n",
      "Two further arrests after men shot and stabbed in Oldham\n",
      "Homes in Bristol evacuated over bomb fears after man arrested\n",
      "Hull drugs raid: Police seize suspected cannabis plants worth £200k\n",
      "Cantwell and Lundstram 'have to start' - gossip\n",
      "Hanlon and Stevenson both approached by Raith Rovers - gossip\n",
      "Chile arrests firefighter for blaze that killed 137 people\n",
      "Snodgrass booed following Naismith criticism - gossip\n",
      "Southampton: Fire crews tackle blaze at Zen Japanese restaurant\n",
      "Make-up and McDonald's: Dangerous driving caught on camera\n",
      "Flamingo Land baby hippo recovering after 'rocky' start\n",
      "Weather warning over rain and thunderstorms across East\n",
      "Poyntzpass: Man in his 60s dies after Newry Road crash \n",
      "Girl's life changed 'forever' after attempted rape in Salisbury\n",
      "NBA play-offs: Dallas Mavericks' Luka Doncic hits game-winner against Minnesota Timberwolves\n",
      "Police move Rangers fans away from Trongate ahead of Celtic march\n",
      "Thunderstorm warning for Wales on Sunday\n",
      "Channel migrants: Crossings top 10,000 so far this year\n",
      "Shropshire Council halved overspend but 'no room for error'\n",
      "Weather forecast for the UK\n",
      "Northern Ireland weather forecast\n",
      "Middle East weather forecast\n",
      "Europe weather forecast\n",
      "North America weather forecast\n",
      "Asia weather forecast\n",
      "Australasia weather forecast\n",
      "Watch Newsround\n",
      "London: Child dies in fall from upper floor flat in Kennington\n",
      "Taylor Swift: How film stars faded, and pop stars took over the planet\n",
      "Jodie Picoult: 'It's not a badge of honour to have books banned'\n",
      "Uvalde families sue Meta, video game creator and gunmaker\n",
      "Josh Taylor v Jack Catterall 2: Predictions for all-British rematch\n",
      "Belfast culture night's return being explored \n",
      "Church of Ireland: 'I'm saddened church rejected baptism changes'\n",
      "Pride festival: Chichester to host first Pride parade\n",
      "Derby's Markeaton Park high ropes to reopen\n",
      "Secamb: Electric vehicles unveiled by ambulance service\n",
      "Oxfordshire's Big Picture: 20 May - 26 May 2024\n",
      "Appleby free shuttle bus launches in bid to boost tourism\n",
      "Two paddling pools reopen for summer season in Devon\n",
      "Volunteer blood bikers prepare for 20,000th life saving delivery \n",
      "Clay Cross: Demolition of 'dated' leisure centre starts\n",
      "Police in Devon offer free security mark for bicycles\n",
      "CBBC's Hacker T Dog celebrates 15 years on screen\n",
      "Wolverhampton roadworks scheme worth £9.7m to begin\n",
      "Derelict Manchester mill could be turned into student flats\n",
      "MollyFest: Final music festival to honour murdered Molly McLaren\n",
      "Guernsey's Harbour Master to retire\n",
      "Ripley: Assistant referee nearly missed call for FA Cup final \n",
      "Takeaway demolished for Worcester's Arches development\n",
      "Gandalf and Girls Aloud feature in Manchester's Flower Festival trail\n",
      "Disused 19th Century properties to be demolished in Alderney\n",
      "Dorset's Big Picture: 20 May - 26 May 2024\n",
      "Festival run group Ravers2Runners kick off first event in Bristol\n",
      "Jersey Muay Thai boxer 'excited' for first Jersey title fight\n",
      "Southampton: Opposition to Dolphin Hotel student housing plans\n",
      "Bereaved men opening up at 'Grieving Pint' sessions\n",
      "Bikers invited to take part in Isle of Man TT course Legacy Lap\n",
      "Toy giraffe missing after RAF Brize Norton charity flight\n",
      "Shoppers in Hexham and Sunderland react to M&S store closures\n",
      "Horsham: Man says he will 'never forget being crushed by hay bales'\n",
      "Cotswold Wildlife Park's Lemur Week marked by 70th breeding success\n",
      "Leicester: Dozens of murals go on display around city\n",
      "Town's first Pride will celebrate 'inclusion' \n",
      "Seahorse find in Poole was one of biggest ever recorded - expert\n",
      "Nottingham: Events in city to mark 200 years since Lord Byron's death\n",
      "Wokingham: New wheelie bins arrive in waste collection change\n",
      "NBA play-offs: Luka Doncic gives Dallas Mavericks 2-0 lead over Minnesota Timberwolves\n",
      "Take That fans share excitement ahead of Nottinghamshire gigs\n",
      "United Ireland: New study challenges cost predictions\n",
      "'I was stalked like in Baby Reindeer'\n",
      "Don't Go To Bed Just Yet: Leeds United podcast on Wembley\n",
      "Trowbridge town crier 'totally lost for words' after award\n",
      "Wiltshire church displays 180 years worth of memorabilia\n",
      "Panacea Society: Centenary marks arrival of 'prophet's cradle'\n",
      "Boaters race to take part in model rally in Wicksteed Park\n",
      "Ongoing West Midlands rail disruption after attempted cable theft \n",
      "'We have nothing to lose' - Hearts eye Scottish Cup upset against Rangers\n",
      "Middlesbrough: Man charged with attempted murder over shooting\n",
      "Scottish gossip: Tavernier, Hanlon & Stevenson, Kilmarnock, Hearts and Aberdeen\n",
      "Jeremy Hunt and Rachel Reeves in election battle over tax cuts\n",
      "Brighton and Hove welcomes lifeguards to beaches for 2024\n",
      "Periods during exams were a 'nightmare'\n",
      "Man charged with terrorism offences after travelling to Syria\n",
      "Rishi Sunak sees the funny side of Downing Street soaking\n",
      "Man charged with arson after fire at Burslem supermarket\n",
      "Aberdeen open contract talks with McGrath - gossip\n",
      "Shropshire hospitals to be revamped as business plan approved\n",
      "Portales 'buzzing' for Dundee derby - gossip\n",
      "Wright dreaming of Europe after 'brilliant' qualification - gossip\n",
      "Your pictures of Scotland: Photographs from around the country\n"
     ]
    }
   ],
   "source": [
    "for i in df_BBC[~df_BBC['Title'].str.contains('weekly round-up', case=False)].drop_duplicates(\"Title\").Title:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex weekly round-up: 18 May - 24 May 2024 \n",
      "Surrey weekly round-up: 18 May - 24 May 2024 \n",
      "Kent weekly round-up: 18 May - 24 May 2024\n"
     ]
    }
   ],
   "source": [
    "for i in df_BBC[df_BBC['Title'].str.contains('weekly round-up', case=False)].Title:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_sport = [\"news\",\"sport\"]\n",
    "# news_sport_urls = df_BBC[df_BBC.Topic.isin(news_sport)]\n",
    "# news_sport_urls.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_elements(input_string: str) -> str:\n",
    "    \"\"\"\n",
    "    This function removes all HTML tags and their content from the input string,\n",
    "    and removes specific patterns such as \"Published X hours ago\" and \"Image source\".\n",
    "    \"\"\"\n",
    "    # Parse the input string as HTML\n",
    "    soup = BeautifulSoup(input_string, 'html.parser')\n",
    "    \n",
    "    # Remove <script>, <style>, and <picture> tags and their content\n",
    "    for script in soup([\"script\", \"style\", \"picture\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Extract text from the parsed HTML\n",
    "    cleaned_string = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    # Define regular expressions to remove unwanted patterns\n",
    "    patterns = [\n",
    "        r'\\bPublished.*?\\bago\\b',  # Matches \"Published X hours ago\"\n",
    "        r'\\bImage source\\b',   # Matches \"Image source, ...\"\n",
    "        r'\\bImage caption\\b',  # Matches \"Image caption, ...\"\n",
    "        r'\\bMedia caption\\b',  # Matches \"Media caption, ...\"\n",
    "        r'\\bGetty Images\\b', \n",
    "        r'\\bBBC Wales News\\b',         # Matches \"BBC Wales News\"\n",
    "        r'\\bPublished\\s\\d{1,2}\\s\\w+\\b',\n",
    "        #r'\\bRelated Topics\\b.*',\n",
    "        r'\\bRelated Internet\\b.*', \n",
    "        r'\\bBBC News Staff\\b',\n",
    "        r'\\bFollow\\s.*?\\snews.*\\b',\n",
    "        r'\\b\\w+/\\w+\\b',\n",
    "        r'Follow\\sBBC.*'           \n",
    " \n",
    "    ]\n",
    "\n",
    "    # Remove the matched patterns from the text\n",
    "    for pattern in patterns:\n",
    "        cleaned_string = re.sub(pattern, '', cleaned_string, flags=re.DOTALL)\n",
    "    \n",
    "    # Additional cleanup: remove extra spaces and newlines\n",
    "    cleaned_string = re.sub(r'[\"\\'.,]+', '', cleaned_string)\n",
    "    cleaned_string = re.sub(r'\\s+', ' ', cleaned_string).strip()\n",
    "    cleaned_string = cleaned_string.replace(\"This video can not be played To play this video you need to enable JavaScript in your browser.\",\"\")\n",
    "    string2 = \"Sign up for our morning newsletter and get BBC News in your inbox.\"\n",
    "    cleaned_string = cleaned_string.replace(string2,\"\")\n",
    "\n",
    "    \n",
    "    article = []\n",
    "    for line in (i for i in cleaned_string.split(\"\\n\") if len(i) >= 10):\n",
    "        article.append(line)\n",
    "\n",
    "    return \" \".join(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def request_sentences_from_urls_async(urls, timeout=20):\n",
    "    articles_df = pd.DataFrame(columns=[\"url\",\"topic\",\"title\",\"content\"])\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for idx, url in enumerate(urls.Url, start=1):\n",
    "            if (idx - 1) % 100 == 0:\n",
    "                logging.info(f\"\\nProcessing URL {((idx - 1)//100)+1}/{(len(urls)//100)+1}\")\n",
    "\n",
    "            tasks.append(fetch_url(session, url, timeout))\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        for idx, (url, result) in enumerate(zip(urls.Url, results), start=1):\n",
    "            if result is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tree = etree.HTML(result)\n",
    "                article_element = tree.find(\".//article\")\n",
    "                if article_element is not None:\n",
    "                    outer_html = etree.tostring(article_element, encoding='unicode')\n",
    "                    article_body = remove_elements(outer_html)\n",
    "                    article = [line for line in article_body.split(\"\\n\") if len(line) >= 40]\n",
    "                    articles_df.loc[idx - 1] = (urls[\"Url\"][idx - 1],urls[\"Topic\"][idx - 1],urls[\"Title\"][idx - 1],\" \".join(article))\n",
    "                else:\n",
    "                    # If no <article> element is found, try using BeautifulSoup with the specific ID\n",
    "                    soup = BeautifulSoup(result, 'html.parser')\n",
    "                    article_id = 'main-content'  # Replace with the actual ID you are targeting\n",
    "                    article_element = soup.find(id=article_id)\n",
    "                    if article_element:\n",
    "                        article_body = remove_elements(str(article_element))\n",
    "                        article = [line for line in article_body.split(\"\\n\") if len(line) >= 40]\n",
    "                        articles_df.loc[idx - 1] = (urls[\"Url\"][idx - 1],urls[\"Topic\"][idx - 1],urls[\"Title\"][idx - 1],\" \".join(article))\n",
    "                    else:\n",
    "                        logging.warning(f\"No article content found on the page with ID {article_id}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error extracting article content from {url}: error: {e}\")\n",
    "\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Processing URL 1/2\n",
      "INFO:root:\n",
      "Processing URL 2/2\n"
     ]
    }
   ],
   "source": [
    "timeout = 20  # Timeout value\n",
    "async def main():\n",
    "    articles = await request_sentences_from_urls_async(df_BBC, timeout)\n",
    "    return articles\n",
    "articles_df = await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeding and similarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amoha\\anaconda3\\envs\\cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\amoha\\anaconda3\\envs\\cuda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DPRQuestionEncoder were not initialized from the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base and are newly initialized: ['bert_model.embeddings.LayerNorm.bias', 'bert_model.embeddings.LayerNorm.weight', 'bert_model.embeddings.position_embeddings.weight', 'bert_model.embeddings.token_type_embeddings.weight', 'bert_model.embeddings.word_embeddings.weight', 'bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.0.attention.output.dense.bias', 'bert_model.encoder.layer.0.attention.output.dense.weight', 'bert_model.encoder.layer.0.attention.self.key.bias', 'bert_model.encoder.layer.0.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.self.query.bias', 'bert_model.encoder.layer.0.attention.self.query.weight', 'bert_model.encoder.layer.0.attention.self.value.bias', 'bert_model.encoder.layer.0.attention.self.value.weight', 'bert_model.encoder.layer.0.intermediate.dense.bias', 'bert_model.encoder.layer.0.intermediate.dense.weight', 'bert_model.encoder.layer.0.output.LayerNorm.bias', 'bert_model.encoder.layer.0.output.LayerNorm.weight', 'bert_model.encoder.layer.0.output.dense.bias', 'bert_model.encoder.layer.0.output.dense.weight', 'bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.1.attention.output.dense.bias', 'bert_model.encoder.layer.1.attention.output.dense.weight', 'bert_model.encoder.layer.1.attention.self.key.bias', 'bert_model.encoder.layer.1.attention.self.key.weight', 'bert_model.encoder.layer.1.attention.self.query.bias', 'bert_model.encoder.layer.1.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.self.value.bias', 'bert_model.encoder.layer.1.attention.self.value.weight', 'bert_model.encoder.layer.1.intermediate.dense.bias', 'bert_model.encoder.layer.1.intermediate.dense.weight', 'bert_model.encoder.layer.1.output.LayerNorm.bias', 'bert_model.encoder.layer.1.output.LayerNorm.weight', 'bert_model.encoder.layer.1.output.dense.bias', 'bert_model.encoder.layer.1.output.dense.weight', 'bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.output.dense.bias', 'bert_model.encoder.layer.10.attention.output.dense.weight', 'bert_model.encoder.layer.10.attention.self.key.bias', 'bert_model.encoder.layer.10.attention.self.key.weight', 'bert_model.encoder.layer.10.attention.self.query.bias', 'bert_model.encoder.layer.10.attention.self.query.weight', 'bert_model.encoder.layer.10.attention.self.value.bias', 'bert_model.encoder.layer.10.attention.self.value.weight', 'bert_model.encoder.layer.10.intermediate.dense.bias', 'bert_model.encoder.layer.10.intermediate.dense.weight', 'bert_model.encoder.layer.10.output.LayerNorm.bias', 'bert_model.encoder.layer.10.output.LayerNorm.weight', 'bert_model.encoder.layer.10.output.dense.bias', 'bert_model.encoder.layer.10.output.dense.weight', 'bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.11.attention.output.dense.bias', 'bert_model.encoder.layer.11.attention.output.dense.weight', 'bert_model.encoder.layer.11.attention.self.key.bias', 'bert_model.encoder.layer.11.attention.self.key.weight', 'bert_model.encoder.layer.11.attention.self.query.bias', 'bert_model.encoder.layer.11.attention.self.query.weight', 'bert_model.encoder.layer.11.attention.self.value.bias', 'bert_model.encoder.layer.11.attention.self.value.weight', 'bert_model.encoder.layer.11.intermediate.dense.bias', 'bert_model.encoder.layer.11.intermediate.dense.weight', 'bert_model.encoder.layer.11.output.LayerNorm.bias', 'bert_model.encoder.layer.11.output.LayerNorm.weight', 'bert_model.encoder.layer.11.output.dense.bias', 'bert_model.encoder.layer.11.output.dense.weight', 'bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.dense.bias', 'bert_model.encoder.layer.2.attention.output.dense.weight', 'bert_model.encoder.layer.2.attention.self.key.bias', 'bert_model.encoder.layer.2.attention.self.key.weight', 'bert_model.encoder.layer.2.attention.self.query.bias', 'bert_model.encoder.layer.2.attention.self.query.weight', 'bert_model.encoder.layer.2.attention.self.value.bias', 'bert_model.encoder.layer.2.attention.self.value.weight', 'bert_model.encoder.layer.2.intermediate.dense.bias', 'bert_model.encoder.layer.2.intermediate.dense.weight', 'bert_model.encoder.layer.2.output.LayerNorm.bias', 'bert_model.encoder.layer.2.output.LayerNorm.weight', 'bert_model.encoder.layer.2.output.dense.bias', 'bert_model.encoder.layer.2.output.dense.weight', 'bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.3.attention.output.dense.bias', 'bert_model.encoder.layer.3.attention.output.dense.weight', 'bert_model.encoder.layer.3.attention.self.key.bias', 'bert_model.encoder.layer.3.attention.self.key.weight', 'bert_model.encoder.layer.3.attention.self.query.bias', 'bert_model.encoder.layer.3.attention.self.query.weight', 'bert_model.encoder.layer.3.attention.self.value.bias', 'bert_model.encoder.layer.3.attention.self.value.weight', 'bert_model.encoder.layer.3.intermediate.dense.bias', 'bert_model.encoder.layer.3.intermediate.dense.weight', 'bert_model.encoder.layer.3.output.LayerNorm.bias', 'bert_model.encoder.layer.3.output.LayerNorm.weight', 'bert_model.encoder.layer.3.output.dense.bias', 'bert_model.encoder.layer.3.output.dense.weight', 'bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.dense.bias', 'bert_model.encoder.layer.4.attention.output.dense.weight', 'bert_model.encoder.layer.4.attention.self.key.bias', 'bert_model.encoder.layer.4.attention.self.key.weight', 'bert_model.encoder.layer.4.attention.self.query.bias', 'bert_model.encoder.layer.4.attention.self.query.weight', 'bert_model.encoder.layer.4.attention.self.value.bias', 'bert_model.encoder.layer.4.attention.self.value.weight', 'bert_model.encoder.layer.4.intermediate.dense.bias', 'bert_model.encoder.layer.4.intermediate.dense.weight', 'bert_model.encoder.layer.4.output.LayerNorm.bias', 'bert_model.encoder.layer.4.output.LayerNorm.weight', 'bert_model.encoder.layer.4.output.dense.bias', 'bert_model.encoder.layer.4.output.dense.weight', 'bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.5.attention.output.dense.bias', 'bert_model.encoder.layer.5.attention.output.dense.weight', 'bert_model.encoder.layer.5.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.self.key.weight', 'bert_model.encoder.layer.5.attention.self.query.bias', 'bert_model.encoder.layer.5.attention.self.query.weight', 'bert_model.encoder.layer.5.attention.self.value.bias', 'bert_model.encoder.layer.5.attention.self.value.weight', 'bert_model.encoder.layer.5.intermediate.dense.bias', 'bert_model.encoder.layer.5.intermediate.dense.weight', 'bert_model.encoder.layer.5.output.LayerNorm.bias', 'bert_model.encoder.layer.5.output.LayerNorm.weight', 'bert_model.encoder.layer.5.output.dense.bias', 'bert_model.encoder.layer.5.output.dense.weight', 'bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.6.attention.output.dense.bias', 'bert_model.encoder.layer.6.attention.output.dense.weight', 'bert_model.encoder.layer.6.attention.self.key.bias', 'bert_model.encoder.layer.6.attention.self.key.weight', 'bert_model.encoder.layer.6.attention.self.query.bias', 'bert_model.encoder.layer.6.attention.self.query.weight', 'bert_model.encoder.layer.6.attention.self.value.bias', 'bert_model.encoder.layer.6.attention.self.value.weight', 'bert_model.encoder.layer.6.intermediate.dense.bias', 'bert_model.encoder.layer.6.intermediate.dense.weight', 'bert_model.encoder.layer.6.output.LayerNorm.bias', 'bert_model.encoder.layer.6.output.LayerNorm.weight', 'bert_model.encoder.layer.6.output.dense.bias', 'bert_model.encoder.layer.6.output.dense.weight', 'bert_model.encoder.layer.7.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.7.attention.output.dense.bias', 'bert_model.encoder.layer.7.attention.output.dense.weight', 'bert_model.encoder.layer.7.attention.self.key.bias', 'bert_model.encoder.layer.7.attention.self.key.weight', 'bert_model.encoder.layer.7.attention.self.query.bias', 'bert_model.encoder.layer.7.attention.self.query.weight', 'bert_model.encoder.layer.7.attention.self.value.bias', 'bert_model.encoder.layer.7.attention.self.value.weight', 'bert_model.encoder.layer.7.intermediate.dense.bias', 'bert_model.encoder.layer.7.intermediate.dense.weight', 'bert_model.encoder.layer.7.output.LayerNorm.bias', 'bert_model.encoder.layer.7.output.LayerNorm.weight', 'bert_model.encoder.layer.7.output.dense.bias', 'bert_model.encoder.layer.7.output.dense.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.dense.bias', 'bert_model.encoder.layer.8.attention.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.key.bias', 'bert_model.encoder.layer.8.attention.self.key.weight', 'bert_model.encoder.layer.8.attention.self.query.bias', 'bert_model.encoder.layer.8.attention.self.query.weight', 'bert_model.encoder.layer.8.attention.self.value.bias', 'bert_model.encoder.layer.8.attention.self.value.weight', 'bert_model.encoder.layer.8.intermediate.dense.bias', 'bert_model.encoder.layer.8.intermediate.dense.weight', 'bert_model.encoder.layer.8.output.LayerNorm.bias', 'bert_model.encoder.layer.8.output.LayerNorm.weight', 'bert_model.encoder.layer.8.output.dense.bias', 'bert_model.encoder.layer.8.output.dense.weight', 'bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.9.attention.output.dense.bias', 'bert_model.encoder.layer.9.attention.output.dense.weight', 'bert_model.encoder.layer.9.attention.self.key.bias', 'bert_model.encoder.layer.9.attention.self.key.weight', 'bert_model.encoder.layer.9.attention.self.query.bias', 'bert_model.encoder.layer.9.attention.self.query.weight', 'bert_model.encoder.layer.9.attention.self.value.bias', 'bert_model.encoder.layer.9.attention.self.value.weight', 'bert_model.encoder.layer.9.intermediate.dense.bias', 'bert_model.encoder.layer.9.intermediate.dense.weight', 'bert_model.encoder.layer.9.output.LayerNorm.bias', 'bert_model.encoder.layer.9.output.LayerNorm.weight', 'bert_model.encoder.layer.9.output.dense.bias', 'bert_model.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "0/18\n",
      "1/18\n",
      "2/18\n",
      "3/18\n",
      "4/18\n",
      "5/18\n",
      "6/18\n",
      "7/18\n",
      "8/18\n",
      "9/18\n",
      "10/18\n",
      "11/18\n",
      "12/18\n",
      "13/18\n",
      "14/18\n",
      "15/18\n",
      "16/18\n",
      "17/18\n",
      "18/18\n",
      "Encoding completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "import re\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "article_main_body = list(articles_df.content)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    encoder = torch.nn.DataParallel(encoder)\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Initialize an empty list to store embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Process and encode each article body\n",
    "for i, data in enumerate(article_main_body):\n",
    "    # Tokenize with padding\n",
    "    inputs = tokenizer(\"\".join(data), return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)  # Move inputs to GPU\n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        embedding = encoder(**inputs).pooler_output\n",
    "    embeddings.append(embedding)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i // 10}/{len(article_main_body) // 10}\")\n",
    "\n",
    "print(\"Encoding completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings tensor to numpy arrays\n",
    "embeddings_np = [embedding.cpu().numpy() for embedding in embeddings]\n",
    "\n",
    "# Convert embeddings to float32\n",
    "embeddings_np = [embedding.astype('float32') for embedding in embeddings_np]\n",
    "embeddings_np = np.array(embeddings_np).reshape(len(embeddings_np), 768)\n",
    "embeddings_list = [embedding for embedding in embeddings_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "articles_df[\"embedding\"] = embeddings_list\n",
    "articles_df = articles_df.drop_duplicates(subset='content', keep='first').reset_index(drop=True)\n",
    "\n",
    "#content_embedding = (list(bbc_news.values()),embeddings_np)\n",
    "\n",
    "with open('content_embedding.pkl', 'wb') as file:\n",
    "    pickle.dump(articles_df, file)\n",
    "\n",
    "print(\"Embeddings saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title 1: 'Tornadoes and storms leave 15 dead across central US'\n",
      "Title 2: 'Papua New Guinea: Thousands feared missing after landslide'\n",
      "Similarity Score: -0.297607421875\n",
      "Title 1: 'Berkshire's Big Picture: 27 May - 2 June 2024'\n",
      "Title 2: 'Dorset's Big Picture: 27 May - 2 June 2024'\n",
      "Similarity Score: 0.46630859375\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the index\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # Assuming embeddings are of the same dimension\n",
    "embedding = [np.array(i) for i in articles_df.embedding]\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(embedding))\n",
    "\n",
    "# Search for similar embeddings\n",
    "D, I = index.search(np.array(embedding), k=2)  # k=2 returns the closest two vectors (including self)\n",
    "\n",
    "high_sim = []\n",
    "titles = list(articles_df.title)\n",
    "\n",
    "# To keep track of which titles have been printed\n",
    "printed_titles = set()\n",
    "\n",
    "for i in range(len(I)):\n",
    "    title1 = titles[i]\n",
    "    closest_index = I[i][1]  # I[i][0] will be the index of itself, so we take the second closest\n",
    "    title2 = titles[closest_index]\n",
    "    similarity_score = 1 - D[i][1]  # cosine similarity is 1 - L2 distance\n",
    "    if title1 not in printed_titles and title2 not in printed_titles and similarity_score >-0.4:\n",
    "        print(f\"Title 1: '{title1}'\\nTitle 2: '{title2}'\\nSimilarity Score: {similarity_score}\")\n",
    "        high_sim.append((title1, title2, similarity_score))\n",
    "        printed_titles.add(title1)\n",
    "        printed_titles.add(title2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try other types of  clustering methods such as :\n",
    "\n",
    "Density-based spatial clustering (DBSC) : a density-based clustering algorithm that works on the assumption that clusters are dense regions in space separated by regions of lower density. It groups 'densely grouped' data points into a single cluster.\n",
    "\n",
    "SPECTURAL CLUSTERING : uses information from the eigenvalues (spectrum) of special matrices (i.e. Affinity Matrix, Degree Matrix and Laplacian Matrix) derived from the graph or the data set.\n",
    "DBA - ALOCATION\n",
    "\n",
    "\n",
    "# Next steps\n",
    "\n",
    "### 1\n",
    "\n",
    "1) **SKY :** some urls are the same article from sky news and sky sports. check if they have same text and remove if they do?\n",
    "\n",
    "2) **SKY & BBC :** Make sure BBC and SKY only have news and sports sections that are important to us. \n",
    "\n",
    "3) choose more websites"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
